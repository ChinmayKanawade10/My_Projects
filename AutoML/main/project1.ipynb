{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbs47siFbgUrFsLS9jdW13"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install bayesian-optimization"],"metadata":{"id":"huQD-PNWVHWv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711384525255,"user_tz":-330,"elapsed":18593,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"46e39df1-5e55-4a5a-99e7-05388fe4157b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bayesian-optimization\n","  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.3.0)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n","from bayes_opt import BayesianOptimization\n","from sklearn.metrics import r2_score, accuracy_score"],"metadata":{"id":"gRl2Uy80U2LW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8TRUXl_dzQ5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711384532784,"user_tz":-330,"elapsed":5694,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"b5c32654-ee7b-4870-db78-ce3d1524c835"},"outputs":[{"output_type":"stream","name":"stdout","text":["Column Names with Indexes:\n","Index 0: ambient_temparature\n","Index 1: exhaust_vaccum\n","Index 2: ambient_pressure\n","Index 3: relative_humidity\n","Index 4: energy_output\n","Enter the index of the target variable column: 4\n","\n","X (features):\n","   ambient_temparature  exhaust_vaccum  ambient_pressure  relative_humidity\n","0                14.96           41.76           1024.07              73.17\n","1                25.18           62.96           1020.04              59.08\n","2                 5.11           39.40           1012.16              92.14\n","3                20.86           57.32           1010.24              76.64\n","4                10.82           37.50           1009.23              96.62\n","\n","y (target variable):\n","0    463.26\n","1    444.37\n","2    488.56\n","3    446.48\n","4    473.90\n","Name: energy_output, dtype: float64\n"]}],"source":["data = pd.read_csv('powerplant_energy_data.csv')\n","\n","print(\"Column Names with Indexes:\")\n","for idx, col_name in enumerate(data.columns):\n","    print(f\"Index {idx}: {col_name}\")\n","target_col_idx = int(input(\"Enter the index of the target variable column: \"))\n","\n","\n","X = data.drop(data.columns[target_col_idx], axis=1)\n","y = data.iloc[:, target_col_idx]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nX (features):\")\n","print(X.head())\n","print(\"\\ny (target variable):\")\n","print(y.head())"]},{"cell_type":"code","source":["#@title Data Preprocessing\n"],"metadata":{"cellView":"form","id":"OsI4JA6MXbK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Model Selection\n","\n","def algorithm_type(x_var, y_var):\n","    from sklearn.utils.multiclass import type_of_target\n","    dtype = y_var.dtype\n","    target_type = type_of_target(y_var)\n","\n","    if dtype == 'object' or target_type == 'binary':\n","      problem_type = 'Classification'\n","      print('Object or Binary target variable detected !')\n","\n","    elif target_type == 'continuous':\n","      problem_type = 'Regression'\n","      print('Continuous target variable detected !')\n","\n","    elif dtype in ['int64','float64'] or target_type in ['multiclass']:\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.23, random_state=42)\n","        rf_classifier = RandomForestClassifier()\n","        rf_regressor = RandomForestRegressor()\n","        rf_classifier.fit(X_train, y_train)\n","        rf_regressor.fit(X_train, y_train)\n","        classifier_score = rf_classifier.score(X_test, y_test)\n","        regressor_score = rf_regressor.score(X_test, y_test)\n","        if classifier_score > regressor_score:\n","            problem_type = 'Classification'\n","        else:\n","            problem_type = 'Regression'\n","        print(f'CS:{classifier_score} , RS:{regressor_score}')\n","\n","    else:\n","        problem_type = input('''Specify problem type manually -\n","        (r for Regression / c for Classification) : ''')\n","\n","    return problem_type\n","\n","\n","def model_analysis(ptype):\n","    if ptype.lower() in ['Regression','regression', 'r']:\n","        models = [\n","            ('DecisionTreeRegressor', DecisionTreeRegressor()),\n","            ('RandomForestRegressor', RandomForestRegressor()),\n","            ('GradientBoostingRegressor', GradientBoostingRegressor())\n","        ]\n","    elif ptype.lower() in ['Classification','classification', 'c']:\n","        models = [\n","            ('DecisionTreeClassifier', DecisionTreeClassifier()),\n","            ('RandomForestClassifier', RandomForestClassifier()),\n","            ('GradientBoostingClassifier', GradientBoostingClassifier())\n","        ]\n","    else:\n","        raise ValueError('Invalid problem type specified.')\n","\n","    return models\n","\n","\n","def model_selection(models, X, y, problem_type, cv=3):\n","    from sklearn.metrics import accuracy_score, r2_score\n","    best_model = None\n","    best_score = float('-inf') if problem_type == 'regression' else 0\n","\n","    for name, model in models:\n","        scoring = 'r2' if problem_type == 'regression' else 'accuracy'\n","        scores = cross_val_score(model, X, y, scoring=scoring, cv=cv)\n","        mean_score = scores.mean()\n","        if problem_type == 'regression' and mean_score > best_score:\n","            best_score = mean_score\n","            best_model = model\n","        elif problem_type == 'classification' and mean_score > best_score:\n","            best_score = mean_score\n","            best_model = model\n","    return best_model\n","\n","\n","algorithm = algorithm_type(X, y)\n","print(f'Algorithm selected : {algorithm}')\n","\n","models = model_analysis(algorithm)\n","print(models)\n","\n","best_model = model_selection(models, X, y, algorithm)\n","print(f'Model selected : {best_model}')"],"metadata":{"id":"Qbflvf_Td0KR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711384601386,"user_tz":-330,"elapsed":11206,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"0017b7ba-ba57-44eb-8e3e-138334c086ce","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Continuous target variable detected !\n","Algorithm selected : regression\n","[('DecisionTreeRegressor', DecisionTreeRegressor()), ('RandomForestRegressor', RandomForestRegressor()), ('GradientBoostingRegressor', GradientBoostingRegressor())]\n","Model selected : RandomForestRegressor()\n"]}]},{"cell_type":"code","source":["#@title HPO - Bayesian Optimization - Regression\n","################################################\n","\n","\n","# Decision Tree Regressor\n","def optimize_dtr(max_depth, min_samples_split, min_samples_leaf):\n","    dtr = DecisionTreeRegressor(max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                 min_samples_leaf=int(min_samples_leaf))\n","    dtr.fit(X_train, y_train)\n","    y_pred = dtr.predict(X_test)\n","    return r2_score(y_test, y_pred)\n","\n","dtr_bounds = {'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","              'min_samples_leaf': (1, 20)}\n","\n","bayes_dtr = BayesianOptimization(f=optimize_dtr, pbounds=dtr_bounds, random_state=42)\n","bayes_dtr.maximize(init_points=10, n_iter=10)\n","\n","print(\"Decision Tree Regressor: Best parameters -\", bayes_dtr.max)\n","print(\"R-squared Score:\", bayes_dtr.max['target'])\n","\n","\n","# Random Forest Regressor\n","def optimize_rfr(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n","    rfr = RandomForestRegressor(n_estimators=int(n_estimators), max_depth=int(max_depth),\n","                                 min_samples_split=int(min_samples_split), min_samples_leaf=int(min_samples_leaf))\n","    rfr.fit(X_train, y_train)\n","    y_pred = rfr.predict(X_test)\n","    return r2_score(y_test, y_pred)\n","\n","rfr_bounds = {'n_estimators': (10, 100), 'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","              'min_samples_leaf': (1, 20)}\n","\n","bayes_rfr = BayesianOptimization(f=optimize_rfr, pbounds=rfr_bounds, random_state=42)\n","bayes_rfr.maximize(init_points=10, n_iter=10)\n","\n","print(\"Random Forest Regressor: Best parameters -\", bayes_rfr.max)\n","print(\"R-squared Score:\", bayes_rfr.max['target'])\n","\n","\n","# Gradient Boosting Regressor\n","def optimize_gbr(n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf, subsample):\n","    gbr = GradientBoostingRegressor(n_estimators=int(n_estimators), learning_rate=learning_rate,\n","                                     max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                     min_samples_leaf=int(min_samples_leaf), subsample=subsample)\n","    gbr.fit(X_train, y_train)\n","    y_pred = gbr.predict(X_test)\n","    return r2_score(y_test, y_pred)\n","\n","gbr_bounds = {'n_estimators': (10, 100), 'learning_rate': (0.001, 1.0), 'max_depth': (1, 50),\n","              'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20), 'subsample': (0.1, 1.0)}\n","\n","bayes_gbr = BayesianOptimization(f=optimize_gbr, pbounds=gbr_bounds, random_state=42)\n","bayes_gbr.maximize(init_points=10, n_iter=10)\n","\n","print(\"GBM Regressor: Best parameters -\", bayes_gbr.max)\n","print(\"R-squared Score:\", bayes_gbr.max['target'])\n"],"metadata":{"cellView":"form","id":"d9ZCpXAxVaCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title HPO - Bayesian Optimization - Classification\n","####################################################\n","\n","\n","# Decision Tree Classifier\n","def optimize_dtc(max_depth, min_samples_split, min_samples_leaf):\n","    dtc = DecisionTreeClassifier(max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                  min_samples_leaf=int(min_samples_leaf))\n","    dtc.fit(X_train, y_train)\n","    y_pred = dtc.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","dtc_bounds = {'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","              'min_samples_leaf': (1, 20)}\n","\n","bayes_dtc = BayesianOptimization(f=optimize_dtc, pbounds=dtc_bounds, random_state=42)\n","bayes_dtc.maximize(init_points=10, n_iter=10)\n","\n","print()\n","\n","# Random Forest Classifier\n","def optimize_rfc(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n","    rfc = RandomForestClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth),\n","                                  min_samples_split=int(min_samples_split), min_samples_leaf=int(min_samples_leaf))\n","    rfc.fit(X_train, y_train)\n","    y_pred = rfc.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","rfc_bounds = {'n_estimators': (10, 100), 'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","              'min_samples_leaf': (1, 20)}\n","\n","bayes_rfc = BayesianOptimization(f=optimize_rfc, pbounds=rfc_bounds, random_state=42)\n","bayes_rfc.maximize(init_points=10, n_iter=10)\n","\n","print()\n","\n","# Gradient Boosting Classifier\n","def optimize_gbc(n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf, subsample):\n","    gbc = GradientBoostingClassifier(n_estimators=int(n_estimators), learning_rate=learning_rate,\n","                                     max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                     min_samples_leaf=int(min_samples_leaf), subsample=subsample)\n","    gbc.fit(X_train, y_train)\n","    y_pred = gbc.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","gbc_bounds = {'n_estimators': (10, 100), 'learning_rate': (0.001, 1.0), 'max_depth': (1, 50),\n","              'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20), 'subsample': (0.1, 1.0)}\n","\n","bayes_gbc = BayesianOptimization(f=optimize_gbc, pbounds=gbc_bounds, random_state=42)\n","bayes_gbc.maximize(init_points=10, n_iter=10)\n","\n","print()\n","\n","print(\"Decision Tree Classifier: Best parameters -\", bayes_dtc.max)\n","print(\"Accuracy:\", bayes_dtc.max['target'])\n","print()\n","print(\"Random Forest Classifier: Best parameters -\", bayes_rfc.max)\n","print(\"Accuracy:\", bayes_rfc.max['target'])\n","print()\n","print(\"GBM Classifier: Best parameters -\", bayes_gbc.max)\n","print(\"Accuracy:\", bayes_gbc.max['target'])"],"metadata":{"cellView":"form","id":"nNY0QAC_YGju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Optimizer Functions\n","\n","# Decision Tree Regressor Optimization\n","def optimize_dtr(X_train, X_test, y_train, y_test):\n","    def dtr_optimizer(max_depth, min_samples_split, min_samples_leaf):\n","        dtr = DecisionTreeRegressor(max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                     min_samples_leaf=int(min_samples_leaf))\n","        dtr.fit(X_train, y_train)\n","        y_pred = dtr.predict(X_test)\n","        return r2_score(y_test, y_pred)\n","\n","    dtr_bounds = {'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","                  'min_samples_leaf': (1, 20)}\n","\n","    bayes_dtr = BayesianOptimization(f=dtr_optimizer, pbounds=dtr_bounds, random_state=42)\n","    bayes_dtr.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_dtr, bayes_dtr.max['target']\n","\n","# Random Forest Regressor Optimization\n","def optimize_rfr(X_train, X_test, y_train, y_test):\n","    def rfr_optimizer(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n","        rfr = RandomForestRegressor(n_estimators=int(n_estimators), max_depth=int(max_depth),\n","                                     min_samples_split=int(min_samples_split), min_samples_leaf=int(min_samples_leaf))\n","        rfr.fit(X_train, y_train)\n","        y_pred = rfr.predict(X_test)\n","        return r2_score(y_test, y_pred)\n","\n","    rfr_bounds = {'n_estimators': (10, 100), 'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","                  'min_samples_leaf': (1, 20)}\n","\n","    bayes_rfr = BayesianOptimization(f=rfr_optimizer, pbounds=rfr_bounds, random_state=42)\n","    bayes_rfr.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_rfr, bayes_rfr.max['target']\n","\n","# Gradient Boosting Regressor Optimization\n","def optimize_gbr(X_train, X_test, y_train, y_test):\n","    def gbr_optimizer(n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf, subsample):\n","        gbr = GradientBoostingRegressor(n_estimators=int(n_estimators), learning_rate=learning_rate,\n","                                         max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                         min_samples_leaf=int(min_samples_leaf), subsample=subsample)\n","        gbr.fit(X_train, y_train)\n","        y_pred = gbr.predict(X_test)\n","        return r2_score(y_test, y_pred)\n","\n","    gbr_bounds = {'n_estimators': (10, 100), 'learning_rate': (0.001, 1.0), 'max_depth': (1, 50),\n","                  'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20), 'subsample': (0.1, 1.0)}\n","\n","    bayes_gbr = BayesianOptimization(f=gbr_optimizer, pbounds=gbr_bounds, random_state=42)\n","    bayes_gbr.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_gbr, bayes_gbr.max['target']\n","\n","\n","# Decision Tree Classifier Optimization\n","def optimize_dtc(X_train, X_test, y_train, y_test):\n","    def optimize_dtc_inner(max_depth, min_samples_split, min_samples_leaf):\n","        dtc = DecisionTreeClassifier(max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                      min_samples_leaf=int(min_samples_leaf))\n","        dtc.fit(X_train, y_train)\n","        y_pred = dtc.predict(X_test)\n","        return accuracy_score(y_test, y_pred)\n","\n","    dtc_bounds = {'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","                  'min_samples_leaf': (1, 20)}\n","\n","    bayes_dtc = BayesianOptimization(f=optimize_dtc_inner, pbounds=dtc_bounds, random_state=42)\n","    bayes_dtc.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_dtc, bayes_dtc.max['target']\n","\n","# Random Forest Classifier Optimization\n","def optimize_rfc(X_train, X_test, y_train, y_test):\n","    def rfc_optimizer(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n","        rfc = RandomForestClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth),\n","                                      min_samples_split=int(min_samples_split), min_samples_leaf=int(min_samples_leaf))\n","        rfc.fit(X_train, y_train)\n","        y_pred = rfc.predict(X_test)\n","        return accuracy_score(y_test, y_pred)\n","\n","    rfc_bounds = {'n_estimators': (10, 100), 'max_depth': (1, 50), 'min_samples_split': (2, 20),\n","                  'min_samples_leaf': (1, 20)}\n","\n","    bayes_rfc = BayesianOptimization(f=rfc_optimizer, pbounds=rfc_bounds, random_state=42)\n","    bayes_rfc.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_rfc, bayes_rfc.max['target']\n","\n","# Gradient Boosting Classifier Optimization\n","def optimize_gbc(X_train, X_test, y_train, y_test):\n","    def gbc_optimizer(n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf, subsample):\n","        gbc = GradientBoostingClassifier(n_estimators=int(n_estimators), learning_rate=learning_rate,\n","                                         max_depth=int(max_depth), min_samples_split=int(min_samples_split),\n","                                         min_samples_leaf=int(min_samples_leaf), subsample=subsample)\n","        gbc.fit(X_train, y_train)\n","        y_pred = gbc.predict(X_test)\n","        return accuracy_score(y_test, y_pred)\n","\n","    gbc_bounds = {'n_estimators': (10, 100), 'learning_rate': (0.001, 1.0), 'max_depth': (1, 50),\n","                  'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20), 'subsample': (0.1, 1.0)}\n","\n","    bayes_gbc = BayesianOptimization(f=gbc_optimizer, pbounds=gbc_bounds, random_state=42)\n","    bayes_gbc.maximize(init_points=10, n_iter=10)\n","\n","    return bayes_gbc, bayes_gbc.max['target']"],"metadata":{"id":"UxNnLTjVoy6Q","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Hyperparameter tuning\n","def optimize_model(algorithm, model, X_train, X_test, y_train, y_test):\n","    print(algorithm)\n","    print(model)\n","\n","    if algorithm in ['Regression','regression']:\n","      if isinstance(model, DecisionTreeRegressor):\n","          optimized_model = optimize_dtr(X_train, X_test, y_train, y_test)\n","      elif isinstance(model, RandomForestRegressor):\n","          optimized_model = optimize_rfr(X_train, X_test, y_train, y_test)\n","      elif isinstance(model, GradientBoostingRegressor):\n","          optimized_model = optimize_gbr(X_train, X_test, y_train, y_test)\n","\n","    elif algorithm in ['Classification','classification']:\n","      if isinstance(model, DecisionTreeClassifier):\n","          optimized_model = optimize_dtc(X_train, X_test, y_train, y_test)\n","      elif isinstance(model, RandomForestClassifier):\n","          optimized_model = optimize_rfc(X_train, X_test, y_train, y_test)\n","      elif isinstance(model, GradientBoostingClassifier):\n","          optimized_model = optimize_gbc(X_train, X_test, y_train, y_test)\n","\n","    else:\n","        print('No model selected !!')\n","        optimized_model = None\n","\n","    return optimized_model\n","\n","final_model, performace = optimize_model(algorithm, best_model, X_train, X_test, y_train, y_test)\n","print(f\"Model : {best_model}\")\n","print(f\"Performance : {performace}\")\n","print(final_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDJC_TY8cqnN","executionInfo":{"status":"ok","timestamp":1711386260874,"user_tz":-330,"elapsed":1236,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"f1492e88-6ca8-4d4d-81bf-c823291c6e6a","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["regression\n","RandomForestRegressor()\n","No model selected !!\n","Model : RandomForestRegressor()\n","Performance : None\n"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5C/R1yH8n7tdtK0Qjd1ZS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9mQ7BZQmbX06"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from sklearn.pipeline import make_pipeline\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, LabelEncoder\n","from scipy.stats import zscore"]},{"cell_type":"code","source":["#@title preprocessing_classes\n","class Duplicates:\n","    def __init__(self, duplicates=True):\n","        self.duplicates = duplicates\n","\n","    def handle(self, df):\n","        if self.duplicates:\n","            df.drop_duplicates(inplace=True, ignore_index=True)\n","        return df\n","\n","class MissingValues:\n","    def __init__(self, missing_num=None, missing_categ=None):\n","        self.missing_num = missing_num\n","        self.missing_categ = missing_categ\n","\n","    def handle(self, df, _n_neighbors=5):\n","        if self.missing_num or self.missing_categ:\n","            if df.isna().sum().sum() != 0:\n","                if self.missing_num:\n","                    df = self._handle_missing_num(df, _n_neighbors)\n","                if self.missing_categ:\n","                    df = self._handle_missing_categ(df, _n_neighbors)\n","        return df\n","\n","    def _handle_missing_num(self, df, _n_neighbors):\n","        num_cols = df.select_dtypes(include=np.number).columns\n","        for col in num_cols:\n","            if self.missing_num in ['auto', 'knn']:  # Use KNN imputation\n","                imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                df[col] = imputer.fit_transform(df[[col]])\n","                df[col] = df[col].round().astype('Int64')\n","        return df\n","\n","    def _handle_missing_categ(self, df, _n_neighbors):\n","        cat_cols = set(df.columns) - set(df.select_dtypes(include=np.number).columns)\n","        for col in cat_cols:\n","            if self.missing_categ in ['auto', 'logreg', 'most_frequent']:\n","                if self.missing_categ == 'most_frequent':\n","                    strategy = self.missing_categ\n","                else:\n","                    strategy = 'constant'\n","                imputer = SimpleImputer(strategy=strategy)\n","                df[col] = imputer.fit_transform(df[[col]])\n","        return df\n","\n","class Outliers:\n","    def __init__(self):\n","        pass\n","\n","    def handle(self, df):\n","        df_outliers = self.detect_outliers(df)\n","        df[df_outliers] = np.nan\n","        df.fillna(df.mean(), inplace=True)\n","        return df\n","\n","    def detect_outliers(self, df):\n","        Q1 = df.quantile(0.25)\n","        Q3 = df.quantile(0.75)\n","        IQR = Q3 - Q1\n","        return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n","\n","\n","class Adjust:\n","    def __init__(self, scaler=None, extract_datetime=False):\n","        self.scaler = scaler\n","        self.extract_datetime = extract_datetime\n","\n","    def handle(self, df):\n","        if self.scaler:\n","            if self.scaler in ['MinMax', 'Standard', 'Robust']:\n","                scaler = preprocessing.__getattribute__(self.scaler+'Scaler')()\n","                df[df.columns] = scaler.fit_transform(df[df.columns])\n","        if self.extract_datetime:\n","            df = self._convert_datetime(df)\n","        return df\n","\n","    def _convert_datetime(self, df):\n","        cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)\n","        for col in cols:\n","            try:\n","                df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n","                if self.extract_datetime != False:\n","                    df = df.join(pd.to_datetime(df[col]).dt.__getattribute__(self.extract_datetime))\n","            except:\n","                pass\n","        return df\n","\n","class EncodeCateg:\n","    def __init__(self, encode_categ=None):\n","        self.encode_categ = encode_categ\n","\n","    def handle(self, df):\n","        if self.encode_categ:\n","            if self.encode_categ == 'auto':\n","                self._auto_encode(df)\n","            elif isinstance(self.encode_categ, list):\n","                for col in self.encode_categ:\n","                    if col in df.columns:\n","                        self._auto_encode(df, col)\n","        return df\n","\n","    def _auto_encode(self, df, col=None):\n","        if col:\n","            if len(df[col].unique()) <= 10:\n","                df = pd.get_dummies(df, columns=[col], prefix=[col])\n","            else:\n","                le = LabelEncoder()\n","                df[col] = le.fit_transform(df[col])\n","        else:\n","            for col in df.select_dtypes(include='object'):\n","                if len(df[col].unique()) <= 10:\n","                    df = pd.get_dummies(df, columns=[col], prefix=[col])\n","                else:\n","                    le = LabelEncoder()\n","                    df[col] = le.fit_transform(df[col])\n","        return df\n"],"metadata":{"id":"RQWpdZpTbZ-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title master_class\n","class DataCleaner:\n","    def __init__(self,\n","                 remove_duplicates=True,\n","                 handle_missing_num=True,\n","                 handle_missing_categ=True,\n","                 handle_outliers=True,\n","                 scale_numerical=True,\n","                 extract_datetime=False,\n","                 encode_categorical=True):\n","        self.remove_duplicates = remove_duplicates\n","        self.handle_missing_num = handle_missing_num\n","        self.handle_missing_categ = handle_missing_categ\n","        self.handle_outliers = handle_outliers\n","        self.scale_numerical = scale_numerical\n","        self.extract_datetime = extract_datetime\n","        self.encode_categorical = encode_categorical\n","\n","\n","    def clean(self, df, _n_neighbors=5):\n","        if self.duplicates:\n","            self._handle_duplicates(df)\n","\n","        if self.missing_num or self.missing_categ:\n","            self._handle_missing_values(df, _n_neighbors)\n","\n","        if self.scaler:\n","            self._handle_scaling(df)\n","\n","        if self.extract_datetime:\n","            self._handle_datetime(df)\n","\n","        if self.encode_categ:\n","            self._handle_categorical_encoding(df)\n","\n","        return df\n","\n","    def _handle_duplicates(self, df):\n","        df.drop_duplicates(inplace=True, ignore_index=True)\n","\n","    def _handle_missing_values(self, df, _n_neighbors):\n","        if self.missing_num:\n","            self._handle_missing_num(df, _n_neighbors)\n","        if self.missing_categ:\n","            self._handle_missing_categ(df)\n","\n","    def _handle_missing_num(self, df, _n_neighbors):\n","        num_cols = df.select_dtypes(include=np.number).columns\n","        for col in num_cols:\n","            if self.missing_num in ['auto', 'knn']:  # Use KNN imputation\n","                imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                df[col] = imputer.fit_transform(df[[col]])\n","                df[col] = df[col].round().astype('Int64')\n","\n","    def _handle_missing_categ(self, df):\n","        cat_cols = set(df.columns) - set(df.select_dtypes(include=np.number).columns)\n","        for col in cat_cols:\n","            if self.missing_categ in ['auto', 'logreg', 'most_frequent']:\n","                if self.missing_categ == 'most_frequent':\n","                    strategy = self.missing_categ\n","                else:\n","                    strategy = 'constant'\n","                imputer = SimpleImputer(strategy=strategy)\n","                df[col] = imputer.fit_transform(df[[col]])\n","\n","    def _handle_scaling(self, df):\n","        if self.scaler in ['minMax', 'standard', 'robust']:\n","            scaler = globals()[self.scaler.capitalize() + 'Scaler']()\n","            df[df.columns] = scaler.fit_transform(df[df.columns])\n","\n","    def _handle_datetime(self, df):\n","        cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)\n","        for col in cols:\n","            try:\n","                df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n","                if self.extract_datetime:\n","                    df = df.join(pd.to_datetime(df[col]).dt.__getattribute__(self.extract_datetime))\n","            except:\n","                pass\n","\n","    def _handle_categorical_encoding(self, df):\n","        if self.encode_categ == 'auto':\n","            self._auto_encode(df)\n","        elif isinstance(self.encode_categ, list):\n","            for col in self.encode_categ:\n","                if col in df.columns:\n","                    self._auto_encode(df, col)\n","\n","    def _auto_encode(self, df, col=None):\n","        if col:\n","            if len(df[col].unique()) <= 10:\n","                df = pd.get_dummies(df, columns=[col], prefix=[col])\n","            else:\n","                le = LabelEncoder()\n","                df[col] = le.fit_transform(df[col])\n","        else:\n","            for col in df.select_dtypes(include='object'):\n","                if len(df[col].unique()) <= 10:\n","                    df = pd.get_dummies(df, columns=[col], prefix=[col])\n","                else:\n","                    le = LabelEncoder()\n","                    df[col] = le.fit_transform(df[col])\n"],"metadata":{"id":"fjOKCn6IbaA2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Load the dataset into a DataFrame\n","# df = pd.read_csv('dp_data1.csv')\n","# df.info()\n","\n","# # Instantiate DataCleaner with desired configuration\n","# cleaner = DataCleaner(remove_duplicates=True,\n","#                       handle_missing_num=True,\n","#                       handle_missing_categ=True,\n","#                       handle_outliers=True,\n","#                       scale_numerical=True,\n","#                       extract_datetime=False,\n","#                       encode_categorical=True)\n","\n","# # Apply data cleaning\n","# cleaned_df = cleaner.clean(df)"],"metadata":{"id":"SXL4fUcBqU6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_data(df, config):\n","    # Instantiate preprocessing classes\n","    duplicates_handler = Duplicates(config.get('handle_duplicates', True))\n","    missing_values_handler = MissingValues(config.get('handle_missing_num'), config.get('handle_missing_categ'))\n","    outliers_handler = Outliers()\n","    adjust_handler = Adjust(config.get('scaler'), config.get('extract_datetime'))\n","    encode_categ_handler = EncodeCateg(config.get('encode_categ'))\n","\n","    # Define preprocessing sequence\n","    preprocessing_steps = [\n","        duplicates_handler,\n","        missing_values_handler,\n","        outliers_handler,\n","        adjust_handler,\n","        encode_categ_handler\n","    ]\n","\n","    # Apply preprocessing steps\n","    cleaned_df = df.copy()\n","    for step in preprocessing_steps:\n","        cleaned_df = step.handle(cleaned_df)\n","\n","    return cleaned_df"],"metadata":{"id":"zS5vQqWh0We7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv('dp_data1.csv')\n","dataset.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8LS5_yy0YBN","executionInfo":{"status":"ok","timestamp":1709578872140,"user_tz":-330,"elapsed":12,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"6028ecfe-8e27-403a-d045-a11028af626a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 25 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Feature_1   1000 non-null   float64\n"," 1   Feature_2   1000 non-null   float64\n"," 2   Feature_3   1000 non-null   float64\n"," 3   Feature_4   1000 non-null   int64  \n"," 4   Feature_5   1000 non-null   int64  \n"," 5   Feature_6   1000 non-null   object \n"," 6   Feature_7   1000 non-null   int64  \n"," 7   Feature_8   1000 non-null   float64\n"," 8   Feature_9   1000 non-null   float64\n"," 9   Feature_10  1000 non-null   float64\n"," 10  Feature_11  1000 non-null   int64  \n"," 11  Feature_12  1000 non-null   float64\n"," 12  Feature_13  1000 non-null   float64\n"," 13  Feature_14  1000 non-null   float64\n"," 14  Feature_15  1000 non-null   int64  \n"," 15  Feature_16  1000 non-null   float64\n"," 16  Feature_17  782 non-null    float64\n"," 17  Feature_18  1000 non-null   float64\n"," 18  Feature_19  746 non-null    object \n"," 19  Feature_20  1000 non-null   float64\n"," 20  Feature_21  1000 non-null   float64\n"," 21  Feature_22  674 non-null    float64\n"," 22  Feature_23  1000 non-null   float64\n"," 23  Feature_24  1000 non-null   float64\n"," 24  Feature_25  1000 non-null   object \n","dtypes: float64(17), int64(5), object(3)\n","memory usage: 195.4+ KB\n"]}]},{"cell_type":"code","source":["config = {\n","    'duplicates': True,\n","    'missing_num': 'auto',\n","    'missing_categ': 'most_frequent',\n","    'handle_outliers': True,\n","    'scaler': 'minMax',\n","    'extract_datetime': 'dayofweek',\n","    'encode_categ': 'auto'\n","}\n","\n","cleaned_df = clean_data(dataset, config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ez0V0Ouu0X94","executionInfo":{"status":"ok","timestamp":1709578939656,"user_tz":-330,"elapsed":814,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"57a3f43a-86e0-433a-fab8-74e20114498d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-39-7f5e62082e27>:57: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  Q1 = df.quantile(0.25)\n","<ipython-input-39-7f5e62082e27>:58: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  Q3 = df.quantile(0.75)\n","<ipython-input-39-7f5e62082e27>:60: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n","  return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n","<ipython-input-39-7f5e62082e27>:53: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df.fillna(df.mean(), inplace=True)\n"]}]},{"cell_type":"code","source":["cleaned_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBZYvfiftGd_","executionInfo":{"status":"ok","timestamp":1709578948707,"user_tz":-330,"elapsed":765,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"410f2446-c47d-4ad8-99f9-fc445220d2d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 25 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Feature_1   1000 non-null   float64\n"," 1   Feature_2   1000 non-null   float64\n"," 2   Feature_3   1000 non-null   float64\n"," 3   Feature_4   1000 non-null   float64\n"," 4   Feature_5   1000 non-null   float64\n"," 5   Feature_6   670 non-null    object \n"," 6   Feature_7   1000 non-null   float64\n"," 7   Feature_8   1000 non-null   float64\n"," 8   Feature_9   1000 non-null   float64\n"," 9   Feature_10  1000 non-null   float64\n"," 10  Feature_11  1000 non-null   float64\n"," 11  Feature_12  1000 non-null   float64\n"," 12  Feature_13  1000 non-null   float64\n"," 13  Feature_14  1000 non-null   float64\n"," 14  Feature_15  1000 non-null   float64\n"," 15  Feature_16  1000 non-null   float64\n"," 16  Feature_17  1000 non-null   float64\n"," 17  Feature_18  1000 non-null   float64\n"," 18  Feature_19  499 non-null    object \n"," 19  Feature_20  1000 non-null   float64\n"," 20  Feature_21  1000 non-null   float64\n"," 21  Feature_22  1000 non-null   float64\n"," 22  Feature_23  1000 non-null   float64\n"," 23  Feature_24  1000 non-null   float64\n"," 24  Feature_25  670 non-null    object \n","dtypes: float64(22), object(3)\n","memory usage: 195.4+ KB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"y66tNW5y6aHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i1xchAr46aiZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from scipy.stats import zscore\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import preprocessing"],"metadata":{"id":"oprIV_DH6aky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Data Cleaning\n","class DataCleaning:\n","    def __init__(self, duplicates=True, missing_num=None, missing_categ=None, outliers_method=None, scaler=None, extract_datetime=False, encode_categ=None):\n","        self.duplicates = duplicates\n","        self.missing_num = missing_num\n","        self.missing_categ = missing_categ\n","        self.outliers_method = outliers_method\n","        self.scaler = scaler\n","        self.extract_datetime = extract_datetime\n","        self.encode_categ = encode_categ\n","\n","    def fit_transform(self, df):\n","        original_dtypes = df.dtypes\n","\n","        if self.duplicates:\n","            df = Duplicates().handle(df)\n","        if self.missing_num or self.missing_categ:\n","            df = MissingValues(self.missing_num, self.missing_categ).handle(df)\n","        if self.outliers_method:\n","            df = Outliers().handle(df)\n","        if self.scaler or self.extract_datetime:\n","            df = Adjust(self.scaler, self.extract_datetime).handle(df)\n","        if self.encode_categ:\n","            df = EncodeCateg(self.encode_categ).handle(df)\n","\n","        for col in df.columns:\n","            if original_dtypes[col] in [np.float64, np.int64]:\n","                df[col] = df[col].astype(original_dtypes[col])\n","\n","        return df\n","\n","\n","class Duplicates:\n","    def handle(self, df):\n","        df.drop_duplicates(inplace=True, ignore_index=True)\n","        return df\n","\n","\n","class MissingValues:\n","    def __init__(self, missing_num=None, missing_categ=None):\n","        self.missing_num = missing_num\n","        self.missing_categ = missing_categ\n","\n","    def handle(self, df, _n_neighbors=5):\n","        if self.missing_num or self.missing_categ:\n","            if df.isna().sum().sum() != 0:\n","                if self.missing_num:\n","                    df = self._handle_missing_num(df, _n_neighbors)\n","                if self.missing_categ:\n","                    df = self._handle_missing_categ(df, _n_neighbors)\n","        return df\n","\n","    def _handle_missing_num(self, df, _n_neighbors):\n","        num_cols = df.select_dtypes(include=np.number).columns\n","        for col in num_cols:\n","            if self.missing_num in ['auto', 'knn']:\n","                imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                df[col] = imputer.fit_transform(df[[col]])\n","                df[col] = df[col].round().astype('Int64')\n","        return df\n","\n","    def _handle_missing_categ(self, df, _n_neighbors):\n","        cat_cols = set(df.columns) - set(df.select_dtypes(include=np.number).columns)\n","        for col in cat_cols:\n","            if self.missing_categ in ['auto', 'logreg', 'most_frequent']:\n","                if self.missing_categ == 'most_frequent':\n","                    strategy = self.missing_categ\n","                else:\n","                    strategy = 'constant'\n","                imputer = SimpleImputer(strategy=strategy)\n","                df[col] = imputer.fit_transform(df[[col]])\n","        return df\n","\n","\n","class Outliers:\n","    def handle(self, df):\n","        df = self.replace_outliers(df)\n","        return df\n","\n","    def detect_outliers(self, df):\n","        Q1 = df.quantile(0.25)\n","        Q3 = df.quantile(0.75)\n","        IQR = Q3 - Q1\n","        return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n","\n","    def replace_outliers(self, df, replacement_value=None):\n","        if replacement_value is None:\n","            replacement_value = df.median(numeric_only=True)\n","        for col in df.columns:\n","            if df[col].dtype != 'O':\n","                Q1 = df[col].quantile(0.25)\n","                Q3 = df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","                df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), replacement_value[col], df[col])\n","        return df\n","\n","\n","class Adjust:\n","    def __init__(self, scaler=None, extract_datetime=False):\n","        self.scaler = scaler\n","        self.extract_datetime = extract_datetime\n","\n","    def handle(self, df):\n","        if self.scaler or self.extract_datetime:\n","            df = self._convert_datetime(df)\n","            if self.scaler:\n","                if self.scaler in ['MinMax', 'Standard', 'Robust']:\n","                    scaler = preprocessing.__getattribute__(self.scaler+'Scaler')()\n","                    df[df.columns] = scaler.fit_transform(df[df.columns])\n","        return df\n","\n","    def _convert_datetime(self, df):\n","        cols = set(df.columns) & set(self.extract_datetime)\n","        for col in cols:\n","            try:\n","                df[col] = pd.to_datetime(df[col], errors='coerce')\n","                if self.extract_datetime:\n","                    df[col + '_year'] = df[col].dt.year\n","                    df[col + '_month'] = df[col].dt.month\n","                    df[col + '_day'] = df[col].dt.day\n","                    df.drop(columns=[col], inplace=True)\n","            except:\n","                pass\n","        return df\n","\n","\n","class EncodeCateg:\n","    def __init__(self, encode_categ=None):\n","        self.encode_categ = encode_categ\n","\n","    def handle(self, df):\n","        if self.encode_categ:\n","            if self.encode_categ == 'auto':\n","                self._auto_encode(df)\n","            elif isinstance(self.encode_categ, list):\n","                for col in self.encode_categ:\n","                    if col in df.columns:\n","                        self._auto_encode(df, col)\n","        return df\n","\n","    def _auto_encode(self, df, col=None):\n","        if col:\n","            if df[col].dtype == 'O':\n","                if len(df[col].unique()) <= 10:\n","                    df[col] = df[col].astype('category')\n","                    df = pd.get_dummies(df, columns=[col], prefix=[col], drop_first=True)\n","                else:\n","                    le = LabelEncoder()\n","                    df[col] = le.fit_transform(df[col])\n","        else:\n","            for col in df.select_dtypes(include='object'):\n","                if len(df[col].unique()) <= 10:\n","                    df[col] = df[col].astype('category')\n","                    df = pd.get_dummies(df, columns=[col], prefix=[col], drop_first=True)\n","                else:\n","                    le = LabelEncoder()\n","                    df[col] = le.fit_transform(df[col])\n","        return df"],"metadata":{"cellView":"form","id":"Cdh6m4bn6aoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"dp_data1.csv\")\n","data.info()"],"metadata":{"id":"B48ajZ55CCfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_data = DataCleaning(duplicates=True,missing_num='knn',missing_categ='most_frequent',outliers_method=True,\n","                                scaler='minMax',extract_datetime='year',encode_categ='auto')\n","\n","\n","cleaned_data = clean_data.fit_transform(data)\n","cleaned_data.info()"],"metadata":{"id":"8tfX75O1B8g3"},"execution_count":null,"outputs":[]}]}
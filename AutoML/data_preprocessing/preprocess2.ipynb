{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSGqYyGN3n01arYeLr/A/c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn import preprocessing\n","from sklearn.impute import KNNImputer, SimpleImputer\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder"],"metadata":{"id":"wb7xd1TO04GA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC1Jh0KUxA0z"},"outputs":[],"source":["class Duplicates:\n","    def __init__(self, duplicates=True):\n","        self.duplicates = duplicates\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X\n","    def handle(self, df):\n","        if self.duplicates:\n","            original_shape = df.shape\n","            df.drop_duplicates(inplace=True, ignore_index=True)\n","            df.reset_index(drop=True, inplace=True)\n","            new_shape = df.shape\n","            count = original_shape[0] - new_shape[0]\n","        return df\n","\n","\n","class MissingValues:\n","    def __init__(self, missing_num=None, missing_categ=None):\n","        self.missing_num = missing_num\n","        self.missing_categ = missing_categ\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X\n","    def handle(self, df, _n_neighbors=5):\n","        if self.missing_num or self.missing_categ:\n","            self.count_missing = df.isna().sum().sum()\n","            if self.count_missing != 0:\n","                df = df.dropna(how='all')\n","                df.reset_index(drop=True)\n","                if self.missing_num: # numeric data\n","                    if self.missing_num == 'auto':\n","                        self.missing_num = 'linreg'\n","                        lr = LinearRegression()\n","                        df = self._lin_regression_impute(df, lr)\n","                        self.missing_num = 'knn'\n","                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                        df = self._impute(df, imputer, type='num')\n","                    elif self.missing_num == 'linreg':\n","                        lr = LinearRegression()\n","                        df = self._lin_regression_impute(df, lr)\n","                    elif self.missing_num == 'knn':\n","                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                        df = self._impute(df, imputer, type='num')\n","                    elif self.missing_num in ['mean', 'median', 'most_frequent']:\n","                        imputer = SimpleImputer(strategy=self.missing_num)\n","                        df = self._impute(df, imputer, type='num')\n","                    elif self.missing_num == 'delete':\n","                        df = self._delete(df, type='num')\n","\n","                if self.missing_categ: # categorical data\n","                    if self.missing_categ == 'auto':\n","                        self.missing_categ = 'logreg'\n","                        lr = LogisticRegression()\n","                        df = self._log_regression_impute(df, lr)\n","                        self.missing_categ = 'knn'\n","                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                        df = self._impute(df, imputer, type='categ')\n","                    elif self.missing_categ == 'logreg':\n","                        lr = LogisticRegression()\n","                        df = self._log_regression_impute(df, lr)\n","                    elif self.missing_categ == 'knn':\n","                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n","                        df = self._impute(df, imputer, type='categ')\n","                    elif self.missing_categ == 'most_frequent':\n","                        imputer = SimpleImputer(strategy=self.missing_categ)\n","                        df = self._impute(df, imputer, type='categ')\n","                    elif self.missing_categ == 'delete':\n","                        df = self._delete(df, type='categ')\n","            else:\n","                pass\n","        else:\n","            pass\n","        return df\n","    def _impute(self, df, imputer, type):\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        if type == 'num':\n","            for feature in df.columns:\n","                if feature in cols_num:\n","                    if df[feature].isna().sum().sum() != 0:\n","                        try:\n","                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n","                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n","\n","                            if (df[feature].fillna(-9999) % 1  == 0).all():\n","                                df[feature] = df_imputed\n","                                df[feature] = df[feature].round()\n","                                df[feature] = df[feature].astype('Int64')\n","                            else:\n","                                df[feature] = df_imputed\n","                            if counter != 0:\n","                                pass\n","                        except:\n","                            pass\n","        else:\n","            for feature in df.columns:\n","                if feature not in cols_num:\n","                    if df[feature].isna().sum()!= 0:\n","                        try:\n","                            mapping = dict()\n","                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n","                            mapping[feature] = mappings\n","                            df[feature] = df[feature].map(mapping[feature])\n","                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])\n","                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n","                            df[feature] = df_imputed\n","                            df[feature] = df[feature].round()\n","                            df[feature] = df[feature].astype('Int64')\n","                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n","                            df[feature] = df[feature].map(mappings_inv)\n","                        except:\n","                            pass\n","        return df\n","    def _lin_regression_impute(self, df, model):\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        mapping = dict()\n","        for feature in df.columns:\n","            if feature not in cols_num:\n","                mappings = {k: i for i, k in enumerate(df[feature])}\n","                mapping[feature] = mappings\n","                df[feature] = df[feature].map(mapping[feature])\n","        for feature in cols_num:\n","            try:\n","                test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n","                train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n","                if len(test_df.index) != 0:\n","                    pipe = make_pipeline(StandardScaler(), model)\n","                    y = np.log(train_df[feature])\n","                    X_train = train_df.drop(feature, axis=1)\n","                    test_df.drop(feature, axis=1, inplace=True)\n","                    try:\n","                        model = pipe.fit(X_train, y)\n","                    except:\n","                        y = train_df[feature]\n","                        model = pipe.fit(X_train, y)\n","                    if (y == train_df[feature]).all():\n","                        pred = model.predict(test_df)\n","                    else:\n","                        pred = np.exp(model.predict(test_df))\n","                    test_df[feature]= pred\n","                    if (df[feature].fillna(-9999) % 1  == 0).all():\n","                        test_df[feature] = test_df[feature].round()\n","                        test_df[feature] = test_df[feature].astype('Int64')\n","                        df[feature].update(test_df[feature])\n","                    else:\n","                        df[feature].update(test_df[feature])\n","            except:\n","                pass\n","        for feature in df.columns:\n","            try:\n","                mappings_inv = {v: k for k, v in mapping[feature].items()}\n","                df[feature] = df[feature].map(mappings_inv)\n","            except:\n","                pass\n","        return df\n","    def _log_regression_impute(self, df, model):\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        mapping = dict()\n","        for feature in df.columns:\n","            if feature not in cols_num:\n","                mappings = {k: i for i, k in enumerate(df[feature])}\n","                mapping[feature] = mappings\n","                df[feature] = df[feature].map(mapping[feature])\n","        for feature in cols_num:\n","            try:\n","                test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n","                train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n","                if len(test_df.index) != 0:\n","                    pipe = make_pipeline(StandardScaler(), model)\n","                    y = train_df[feature].astype('int')\n","                    X_train = train_df.drop(feature, axis=1)\n","                    test_df.drop(feature, axis=1, inplace=True)\n","                    try:\n","                        model = pipe.fit(X_train, y)\n","                    except:\n","                        y = np.log(train_df[feature].astype('int'))\n","                        model = pipe.fit(X_train, y)\n","                    if (y == np.log(train_df[feature].astype('int'))).all():\n","                        pred = model.predict(test_df)\n","                    else:\n","                        pred = np.exp(model.predict(test_df))\n","                    test_df[feature]= pred\n","                    if (df[feature].fillna(-9999) % 1  == 0).all():\n","                        test_df[feature] = test_df[feature].round()\n","                        test_df[feature] = test_df[feature].astype('Int64')\n","                        df[feature].update(test_df[feature])\n","                    else:\n","                        df[feature].update(test_df[feature])\n","            except:\n","                pass\n","        for feature in df.columns:\n","            try:\n","                mappings_inv = {v: k for k, v in mapping[feature].items()}\n","                df[feature] = df[feature].map(mappings_inv)\n","            except:\n","                pass\n","        return df\n","    def _delete(self, df, type):\n","        if type == 'num':\n","            cols_num = df.select_dtypes(include=np.number).columns\n","            for feature in df.columns:\n","                if feature in cols_num:\n","                    df = df.dropna(subset=[feature])\n","        else:\n","            for feature in df.columns:\n","                if feature not in cols_num:\n","                    df = df.dropna(subset=[feature])\n","        return df\n","\n","\n","from scipy.stats import zscore\n","class Outliers:\n","    def __init__(self, method=None):\n","        self.method = method\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X\n","    def handle(self, df):\n","        if self.method:\n","            self.count_outliers = 0\n","            if self.method == 'zscore':\n","                df, self.count_outliers = self._zscore_outliers(df)\n","            elif self.method == 'iqr':\n","                df, self.count_outliers = self._iqr_outliers(df)\n","            elif self.method == 'manual':\n","                df, self.count_outliers = self._manual_outliers(df)\n","            else:\n","                pass\n","        return df\n","    def _zscore_outliers(self, df):\n","        outlier_count = 0\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        for feature in cols_num:\n","            if df[feature].dtype != 'object':\n","                z_scores = zscore(df[feature])\n","                abs_z_scores = np.abs(z_scores)\n","                outliers = (abs_z_scores > self.threshold).sum()\n","                outlier_count += outliers\n","                df = df[(abs_z_scores < self.threshold).all(axis=1)]\n","        return df, outlier_count\n","    def _iqr_outliers(self, df):\n","        outlier_count = 0\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        for feature in cols_num:\n","            if df[feature].dtype != 'object':\n","                q1 = df[feature].quantile(0.25)\n","                q3 = df[feature].quantile(0.75)\n","                iqr = q3 - q1\n","                lower_bound = q1 - (self.threshold * iqr)\n","                upper_bound = q3 + (self.threshold * iqr)\n","                outliers = ((df[feature] < lower_bound) | (df[feature] > upper_bound)).sum()\n","                outlier_count += outliers\n","                df = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n","        return df, outlier_count\n","    def _manual_outliers(self, df):\n","        outlier_count = 0\n","        for feature in self.manual_dict:\n","            if feature in df.columns:\n","                outliers = df[df[feature].isin(self.manual_dict[feature])]\n","                outlier_count += len(outliers)\n","                df = df[~df[feature].isin(self.manual_dict[feature])]\n","        return df, outlier_count\n","\n","\n","class Adjust:\n","    def __init__(self, scaler=None, extract_datetime=False, round_values=False):\n","        self.scaler = scaler\n","        self.extract_datetime = extract_datetime\n","        self.round_values = round_values\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X\n","    def handle(self, df):\n","        if self.scaler == 'minmax':\n","            scaler = preprocessing.MinMaxScaler()\n","        elif self.scaler == 'standard':\n","            scaler = preprocessing.StandardScaler()\n","        elif self.scaler == 'robust':\n","            scaler = preprocessing.RobustScaler()\n","        elif self.scaler == 'maxabs':\n","            scaler = preprocessing.MaxAbsScaler()\n","        elif self.scaler == 'quantile':\n","            scaler = preprocessing.QuantileTransformer()\n","        elif self.scaler == 'power':\n","            scaler = preprocessing.PowerTransformer()\n","        if self.extract_datetime:\n","            df = self._convert_datetime(df)\n","        if self.round_values:\n","            df = self._round_values(df)\n","        df[df.columns] = scaler.fit_transform(df[df.columns])\n","        return df\n","    def _convert_datetime(self, df):\n","        cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)\n","        for feature in cols:\n","            try:\n","                df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)\n","                try:\n","                    df['Day'] = pd.to_datetime(df[feature]).dt.day\n","                    if self.extract_datetime in ['auto', 'M','Y','h','m','s']:\n","                        df['Month'] = pd.to_datetime(df[feature]).dt.month\n","                        if self.extract_datetime in ['auto', 'Y','h','m','s']:\n","                            df['Year'] = pd.to_datetime(df[feature]).dt.year\n","                            if self.extract_datetime in ['auto', 'h','m','s']:\n","                                df['Hour'] = pd.to_datetime(df[feature]).dt.hour\n","                                if self.extract_datetime in ['auto', 'm','s']:\n","                                    df['Minute'] = pd.to_datetime(df[feature]).dt.minute\n","                                    if self.extract_datetime in ['auto', 's']:\n","                                        df['Sec'] = pd.to_datetime(df[feature]).dt.second\n","                except:\n","                    pass\n","            except:\n","                pass\n","        return df\n","    def _round_values(self, df):\n","        cols_num = df.select_dtypes(include=np.number).columns\n","        for feature in cols_num:\n","            if (df[feature].fillna(-9999) % 1  == 0).all():\n","                try:\n","                    df[feature] = df[feature].astype('Int64')\n","                except:\n","                    pass\n","            else:\n","                try:\n","                    df[feature] = df[feature].astype(float)\n","                    dec = None\n","                    for value in df[feature]:\n","                        try:\n","                            if dec is None:\n","                                dec = str(value)[::-1].find('.')\n","                            else:\n","                                if str(value)[::-1].find('.') > dec:\n","                                    dec = str(value)[::-1].find('.')\n","                        except:\n","                            pass\n","                    df[feature] = df[feature].round(decimals=dec)\n","                except:\n","                    pass\n","        return df\n","\n","\n","class EncodeCateg:\n","    def __init__(self, encode_categ=None):\n","        self.encode_categ = encode_categ\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return X\n","    def handle(self, df):\n","        if self.encode_categ:\n","            if not isinstance(self.encode_categ, list):\n","                self.encode_categ = ['auto']\n","            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=pd.np.number).columns)\n","            if len(self.encode_categ) == 1:\n","                target_cols = cols_categ\n","            else:\n","                target_cols = self.encode_categ[1]\n","            for feature in target_cols:\n","                if feature in cols_categ:\n","                    feature = feature\n","                else:\n","                    feature = df.columns[feature]\n","                try:\n","                    pd.to_datetime(df[feature])\n","                except:\n","                    try:\n","                        if self.encode_categ[0] == 'auto':\n","                            if df[feature].nunique() <=10:\n","                                df = self._to_onehot(df, feature)\n","                            elif df[feature].nunique() <=20:\n","                                df = self._to_label(df, feature)\n","                        elif self.encode_categ[0] == 'onehot':\n","                            df = self._to_onehot(df, feature)\n","                        elif self.encode_categ[0] == 'label':\n","                            df = self._to_label(df, feature)\n","                    except:\n","                        pass\n","        return df\n","    def _to_onehot(self, df, feature, limit=10):\n","        one_hot = pd.get_dummies(df[feature], prefix=feature)\n","        if one_hot.shape[1] > limit:\n","            print('ONEHOT encoding for feature \"{}\" creates {} new features. Consider LABEL encoding instead.'.format(feature, one_hot.shape[1]))\n","        df = df.join(one_hot)\n","        return df\n","    def _to_label(self, df, feature):\n","        le = LabelEncoder()\n","        df[feature + '_lab'] = le.fit_transform(df[feature].values)\n","        mapping = dict(zip(le.classes_, range(len(le.classes_))))\n","        for key in mapping:\n","            try:\n","                if pd.np.isnan(key):\n","                    replace = {mapping[key] : key }\n","                    df[feature].replace(replace, inplace=True)\n","            except:\n","                pass\n","        return df"]},{"cell_type":"code","source":["class CleanData:\n","    def __init__(self, input_data, mode='auto', duplicates=False, missing_num=False, missing_categ=False,\n","                 encode_categ=False, extract_datetime=False, outliers=False, outlier_param=1.5):\n","        self.input_data = input_data\n","        output_data = input_data.copy()\n","        duplicates, missing_num, missing_categ, outliers, encode_categ, extract_datetime = 'auto', 'auto', 'auto', 'winz', ['auto'], 's'\n","        self.mode = mode\n","        self.duplicates = duplicates\n","        self.missing_num = missing_num\n","        self.missing_categ = missing_categ\n","        self.outliers = outliers\n","        self.encode_categ = encode_categ\n","        self.extract_datetime = extract_datetime\n","        self.outlier_param = outlier_param\n","\n","    def _clean_data(self, input_data):\n","        df = self.input_data.reset_index(drop=True)\n","        df = Duplicates.handle(self, df)\n","        df = MissingValues.handle(self, df)\n","        df = Outliers.handle(self, df)\n","        df = Adjust.convert_datetime(self, df)\n","        df = EncodeCateg.handle(self, df)\n","        df = Adjust.round_values(self, df, self.input_data)\n","        return df"],"metadata":{"id":"bnNPcS0lzOcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv('dp_data1.csv')\n","cleaner = CleanData(input_data=dataset, mode='auto', duplicates=True, missing_num=True, missing_categ=True,\n","                    outliers=True, encode_categ=True, extract_datetime=True)\n","cleaned_data = cleaner._clean_data(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"Jsk8pUNA0zWH","executionInfo":{"status":"error","timestamp":1709461943346,"user_tz":-330,"elapsed":13,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"118915af-104c-4f24-c69c-8aecd67ee0a0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'CleanData' object has no attribute '_lin_regression_impute'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-85d69e715361>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m cleaner = CleanData(input_data=dataset, mode='auto', duplicates=True, missing_num=True, missing_categ=True, \n\u001b[1;32m      3\u001b[0m                     outliers=True, encode_categ=True, extract_datetime=True)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcleaned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-48-7660f5b2ae02>\u001b[0m in \u001b[0;36m_clean_data\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDuplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMissingValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdjust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-9ce855051daa>\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, df, _n_neighbors)\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linreg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lin_regression_impute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'knn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_n_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'CleanData' object has no attribute '_lin_regression_impute'"]}]},{"cell_type":"code","source":["dataset = pd.read_csv('dp_data1.csv')\n","dataset\n","\n","print(\"Column indexes:\")\n","for i, column in enumerate(dataset.columns):\n","    print(f\"{i} : {column}\")\n","\n","target_index = int(input(\"Enter the index of the target variable: \"))\n","feature_indexes_str = input(\"Enter the indexes of the features (comma-separated): \")\n","feature_indexes = [int(idx.strip()) for idx in feature_indexes_str.split(',')]\n","\n","target_variable = dataset.columns[target_index]\n","features = [dataset.columns[idx] for idx in feature_indexes]\n","print()\n","\n","data = dataset[features + [target_variable]]\n","data"],"metadata":{"id":"qsXUervuFfMh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709487645336,"user_tz":-330,"elapsed":815,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"c8a8093b-5509-4a07-b953-d2b0eb873a17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 25 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Feature_1   1000 non-null   float64\n"," 1   Feature_2   1000 non-null   float64\n"," 2   Feature_3   1000 non-null   float64\n"," 3   Feature_4   1000 non-null   int64  \n"," 4   Feature_5   1000 non-null   int64  \n"," 5   Feature_6   1000 non-null   object \n"," 6   Feature_7   1000 non-null   int64  \n"," 7   Feature_8   1000 non-null   float64\n"," 8   Feature_9   1000 non-null   float64\n"," 9   Feature_10  1000 non-null   float64\n"," 10  Feature_11  1000 non-null   int64  \n"," 11  Feature_12  1000 non-null   float64\n"," 12  Feature_13  1000 non-null   float64\n"," 13  Feature_14  1000 non-null   float64\n"," 14  Feature_15  1000 non-null   int64  \n"," 15  Feature_16  1000 non-null   float64\n"," 16  Feature_17  782 non-null    float64\n"," 17  Feature_18  1000 non-null   float64\n"," 18  Feature_19  746 non-null    object \n"," 19  Feature_20  1000 non-null   float64\n"," 20  Feature_21  1000 non-null   float64\n"," 21  Feature_22  674 non-null    float64\n"," 22  Feature_23  1000 non-null   float64\n"," 23  Feature_24  1000 non-null   float64\n"," 24  Feature_25  1000 non-null   object \n","dtypes: float64(17), int64(5), object(3)\n","memory usage: 195.4+ KB\n","None\n"]}]},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","preprocessing_pipeline = Pipeline([\n","    ('missing_values', MissingValues(missing_num='knn', missing_categ='logreg')),\n","    ('outliers', Outliers(method='zscore')),\n","    ('adjust', Adjust(scaler='standard', extract_datetime=True, round_values=True)),\n","    ('encode_categ', EncodeCateg(encode_categ=['auto'])),\n","    ('duplicates', Duplicates())\n","])\n","\n","cleaned_dataset = preprocessing_pipeline.fit_transform(dataset)\n","cleaned_dataset.info()"],"metadata":{"id":"7bzBGLKA15Q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709487651742,"user_tz":-330,"elapsed":723,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"d0f8d21f-7d92-455a-b1b2-534992474195"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 25 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Feature_1   1000 non-null   float64\n"," 1   Feature_2   1000 non-null   float64\n"," 2   Feature_3   1000 non-null   float64\n"," 3   Feature_4   1000 non-null   int64  \n"," 4   Feature_5   1000 non-null   int64  \n"," 5   Feature_6   1000 non-null   object \n"," 6   Feature_7   1000 non-null   int64  \n"," 7   Feature_8   1000 non-null   float64\n"," 8   Feature_9   1000 non-null   float64\n"," 9   Feature_10  1000 non-null   float64\n"," 10  Feature_11  1000 non-null   int64  \n"," 11  Feature_12  1000 non-null   float64\n"," 12  Feature_13  1000 non-null   float64\n"," 13  Feature_14  1000 non-null   float64\n"," 14  Feature_15  1000 non-null   int64  \n"," 15  Feature_16  1000 non-null   float64\n"," 16  Feature_17  782 non-null    float64\n"," 17  Feature_18  1000 non-null   float64\n"," 18  Feature_19  746 non-null    object \n"," 19  Feature_20  1000 non-null   float64\n"," 20  Feature_21  1000 non-null   float64\n"," 21  Feature_22  674 non-null    float64\n"," 22  Feature_23  1000 non-null   float64\n"," 23  Feature_24  1000 non-null   float64\n"," 24  Feature_25  1000 non-null   object \n","dtypes: float64(17), int64(5), object(3)\n","memory usage: 195.4+ KB\n"]}]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.model_selection import cross_val_score"],"metadata":{"id":"QhWbOpYEGW1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ML models\n","regression_models = [LinearRegression, DecisionTreeRegressor, RandomForestRegressor]\n","classification_models = [LogisticRegression, DecisionTreeClassifier, RandomForestClassifier]\n","\n","#problem selection\n","target_dtype = data[target_variable].dtype\n","\n","models = None\n","if target_dtype in [np.float64, np.int64]:\n","    models = regression_models\n","elif target_dtype == np.object:\n","    models = classification_models\n","else:\n","    raise ValueError(\"Unsupported target variable type. Please ensure the target variable is numeric or categorical.\")"],"metadata":{"id":"pmjEv0eGFlxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model = None\n","best_score = -float('inf')\n","\n","for model in models:\n","    scores = cross_val_score(model, cleaned_dataset.drop(columns=[target_variable]), cleaned_dataset[target_variable], cv=5)\n","    avg_score = scores.mean()\n","    if avg_score > best_score:\n","        best_score = avg_score\n","        best_model = model\n","\n","print(f\"The best model is {best_model.__class__.__name__} with an average cross-validation score of {best_score:.2f}.\")"],"metadata":{"id":"pybg3y7HFzEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","\n","# Create features representing characteristics of the dataset and models\n","features = np.column_stack([\n","    X.mean(axis=1),\n","    X.var(axis=1),\n","    X.min(axis=1),\n","    X.max(axis=1),\n","    X.mean() / X.var(),\n","])\n","\n","# Create labels representing the performance of each model\n","# For regression, use mean squared error; for classification, use accuracy\n","labels_regression = np.array([\n","    mean_squared_error(y_regression, LinearRegression().fit(X, y_regression).predict(X)),\n","    mean_squared_error(y_regression, PolynomialFeatures(degree=2).fit_transform(X)),\n","    mean_squared_error(y_regression, DecisionTreeRegressor().fit(X, y_regression).predict(X)),\n","])\n","\n","labels_classification = np.array([\n","    accuracy_score(y_classification, LogisticRegression().fit(X, y_classification).predict(X)),\n","    accuracy_score(y_classification, DecisionTreeClassifier().fit(X, y_classification).predict(X)),\n","])\n","\n","# Split data for training and testing\n","X_train, X_test, y_train, y_test = train_test_split(features, labels_regression, test_size=0.2, random_state=42)\n","\n","# Build a simple neural network\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(16, activation='relu', input_shape=(features.shape[1],)),\n","    tf.keras.layers.Dense(8, activation='relu'),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=100, batch_size=8, validation_data=(X_test, y_test))\n","\n","# Use the trained neural network to predict the best model for a new dataset\n","new_data = np.array([[X.mean(), X.var(), X.min(), X.max(), X.mean() / X.var()]])\n","predicted_performance = model.predict(new_data)\n","\n","# Identify the model with the lowest predicted performance (mean squared error)\n","best_model_index = np.argmin(predicted_performance)\n","best_model = [\"Linear Regression\", \"Polynomial Regression\", \"Decision Tree Regression\"][best_model_index]\n","\n","print(f\"The best fitting model for the given dataset is: {best_model}\")\n"],"metadata":{"id":"CnWi7QnrWiDO"},"execution_count":null,"outputs":[]}]}
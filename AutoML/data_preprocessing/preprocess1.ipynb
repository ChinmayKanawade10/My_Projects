{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfebWBnJpxfOsgJrVoWTY/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4nNXDi4INQ8a"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"],"metadata":{"id":"x1b-Z08ZXqkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv('audiA1_price_data.csv')\n","dataset\n","\n","print(\"Column indexes:\")\n","for i, column in enumerate(dataset.columns):\n","    print(f\"{i} : {column}\")\n","\n","target_index = int(input(\"Enter the index of the target variable: \"))\n","feature_indexes_str = input(\"Enter the indexes of the features (comma-separated): \")\n","feature_indexes = [int(idx.strip()) for idx in feature_indexes_str.split(',')]\n","\n","target_variable = dataset.columns[target_index]\n","features = [dataset.columns[idx] for idx in feature_indexes]\n","print()\n","\n","data = dataset[features + [target_variable]]\n","data"],"metadata":{"id":"JMbvbskm5Xkb","colab":{"base_uri":"https://localhost:8080/","height":767},"executionInfo":{"status":"ok","timestamp":1709123611813,"user_tz":-330,"elapsed":34205,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"af240101-af76-4404-c2b0-60f480f2bfac"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Column indexes:\n","0 : index\n","1 : Year\n","2 : Type\n","3 : Mileage(miles)\n","4 : Engine\n","5 : PS\n","6 : Transmission\n","7 : Fuel\n","8 : Number_of_Owners\n","9 : Price(£)\n","10 : href\n","11 : PPY\n","12 : MileageRank\n","13 : PriceRank\n","14 : PPYRank\n","15 : Score\n","Enter the index of the target variable: 9\n","Enter the indexes of the features (comma-separated): 3,5,6,7,8,11\n"]},{"output_type":"execute_result","data":{"text/plain":["     Mileage(miles)          PS Transmission    Fuel  Number_of_Owners  \\\n","0           44000.0  114.398422       Manual  Diesel                 1   \n","1           42596.0   93.688363       Manual  Petrol                 3   \n","2           42700.0  123.274162       Manual  Petrol                 2   \n","3           86000.0  103.550296       Manual  Diesel                 3   \n","4          104310.0  103.550296       Manual  Diesel                 3   \n","..              ...         ...          ...     ...               ...   \n","466         40195.0  138.067061    Automatic  Petrol                 3   \n","467         26218.0  114.398422       Manual  Petrol                 1   \n","468         48571.0  123.274162       Manual  Petrol                 2   \n","469          9584.0   93.688363       Manual  Petrol                 1   \n","470         17303.0  114.398422       Manual  Petrol                 1   \n","\n","             PPY  Price(£)  \n","0    2499.166667   14995.0  \n","1    2688.750000   10755.0  \n","2    3599.666667   10799.0  \n","3    3745.000000    7490.0  \n","4    3700.000000    7400.0  \n","..           ...       ...  \n","466  5637.500000   11275.0  \n","467  2311.875000   18495.0  \n","468  2399.000000   11995.0  \n","469  2496.250000   19970.0  \n","470  2456.428571   17195.0  \n","\n","[471 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-212bf15a-5c7d-4d25-814e-b2d909b618d9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Mileage(miles)</th>\n","      <th>PS</th>\n","      <th>Transmission</th>\n","      <th>Fuel</th>\n","      <th>Number_of_Owners</th>\n","      <th>PPY</th>\n","      <th>Price(£)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>44000.0</td>\n","      <td>114.398422</td>\n","      <td>Manual</td>\n","      <td>Diesel</td>\n","      <td>1</td>\n","      <td>2499.166667</td>\n","      <td>14995.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>42596.0</td>\n","      <td>93.688363</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>3</td>\n","      <td>2688.750000</td>\n","      <td>10755.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>42700.0</td>\n","      <td>123.274162</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>2</td>\n","      <td>3599.666667</td>\n","      <td>10799.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>86000.0</td>\n","      <td>103.550296</td>\n","      <td>Manual</td>\n","      <td>Diesel</td>\n","      <td>3</td>\n","      <td>3745.000000</td>\n","      <td>7490.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>104310.0</td>\n","      <td>103.550296</td>\n","      <td>Manual</td>\n","      <td>Diesel</td>\n","      <td>3</td>\n","      <td>3700.000000</td>\n","      <td>7400.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>466</th>\n","      <td>40195.0</td>\n","      <td>138.067061</td>\n","      <td>Automatic</td>\n","      <td>Petrol</td>\n","      <td>3</td>\n","      <td>5637.500000</td>\n","      <td>11275.0</td>\n","    </tr>\n","    <tr>\n","      <th>467</th>\n","      <td>26218.0</td>\n","      <td>114.398422</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>1</td>\n","      <td>2311.875000</td>\n","      <td>18495.0</td>\n","    </tr>\n","    <tr>\n","      <th>468</th>\n","      <td>48571.0</td>\n","      <td>123.274162</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>2</td>\n","      <td>2399.000000</td>\n","      <td>11995.0</td>\n","    </tr>\n","    <tr>\n","      <th>469</th>\n","      <td>9584.0</td>\n","      <td>93.688363</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>1</td>\n","      <td>2496.250000</td>\n","      <td>19970.0</td>\n","    </tr>\n","    <tr>\n","      <th>470</th>\n","      <td>17303.0</td>\n","      <td>114.398422</td>\n","      <td>Manual</td>\n","      <td>Petrol</td>\n","      <td>1</td>\n","      <td>2456.428571</td>\n","      <td>17195.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>471 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-212bf15a-5c7d-4d25-814e-b2d909b618d9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-212bf15a-5c7d-4d25-814e-b2d909b618d9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-212bf15a-5c7d-4d25-814e-b2d909b618d9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a02bb71b-514d-49f0-ae8a-20adc5fef574\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a02bb71b-514d-49f0-ae8a-20adc5fef574')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a02bb71b-514d-49f0-ae8a-20adc5fef574 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_10e15906-37b2-4763-8810-51ac9a0e8fd7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_10e15906-37b2-4763-8810-51ac9a0e8fd7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('data');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 471,\n  \"fields\": [\n    {\n      \"column\": \"Mileage(miles)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27020.236123584826,\n        \"min\": 9.0,\n        \"max\": 149000.0,\n        \"num_unique_values\": 416,\n        \"samples\": [\n          25822.0,\n          24950.0,\n          17126.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.345033812546585,\n        \"min\": 84.81262327416174,\n        \"max\": 197.23865877712032,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          93.0,\n          150.0,\n          182.4457593688363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transmission\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Automatic\",\n          \"Manual\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Petrol\",\n          \"Diesel\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number_of_Owners\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 7,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PPY\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 896.64726539429,\n        \"min\": 1682.3333333333333,\n        \"max\": 7375.0,\n        \"num_unique_values\": 406,\n        \"samples\": [\n          2571.428571428572,\n          3166.555555555556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price(\\u00a3)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4955.040913734493,\n        \"min\": 5000.0,\n        \"max\": 30450.0,\n        \"num_unique_values\": 356,\n        \"samples\": [\n          25995.0,\n          9490.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x = data.drop(columns=[target_variable])\n","y = data[target_variable]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)"],"metadata":{"id":"tI3bwq24qdMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #ML models\n","# regression_models = [LinearRegression, DecisionTreeRegressor, RandomForestRegressor]\n","# classification_models = [LogisticRegression, DecisionTreeClassifier, RandomForestClassifier]\n","\n","# #problem selection\n","# target_dtype = data[target_variable].dtype\n","\n","# if target_dtype in [np.float64, np.int64]:\n","#     model = regression_models\n","# elif target_dtype == np.object:\n","#     model = classification_models\n","# else:\n","#     raise ValueError(\"Unsupported target variable type. Please ensure the target variable is numeric or categorical.\")"],"metadata":{"id":"qnQc7PH4OPug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.feature_selection import RFE\n","# def feature_selection(data, target_variable, num_features):\n","#     X = data.drop(columns=[target_variable])\n","#     y = data[target_variable]\n","#     model = LinearRegression()  # You can choose any model for feature selection\n","#     rfe = RFE(model, num_features)\n","#     selected_features = rfe.fit_transform(X, y)\n","#     selected_feature_indices = rfe.get_support(indices=True)\n","#     selected_features_df = X.iloc[:, selected_feature_indices]\n","#     return selected_features_df"],"metadata":{"id":"sRba2N4ZSiNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","from scipy.stats import boxcox\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.decomposition import PCA\n","\n","# Handle missing values\n","def mean_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.mean())\n","    else:\n","        return data\n","\n","def median_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.median())\n","    else:\n","        return data\n","\n","def forward_fill(data):\n","    if data.isnull().any().any():\n","        return data.ffill()\n","    else:\n","        return data\n","\n","def backward_fill(data):\n","    if data.isnull().any().any():\n","        return data.bfill()\n","    else:\n","        return data\n","\n","def delete_missing_values(data):\n","    if data.isnull().any().any():\n","        return data.dropna()\n","    else:\n","        return data\n","\n","# Handle outliers and/or skewness\n","def winsorize(data, lower_percentile=5, upper_percentile=95):\n","    if any(data.apply(lambda x: np.any(x < np.percentile(x, lower_percentile)) or np.any(x > np.percentile(x, upper_percentile)))):\n","        lower_bound = np.percentile(data, lower_percentile)\n","        upper_bound = np.percentile(data, upper_percentile)\n","        data[data < lower_bound] = lower_bound\n","        data[data > upper_bound] = upper_bound\n","        return data\n","    else:\n","        return data\n","\n","def log_transform(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Log transform cannot be applied to non-positive values.\")\n","    return np.log1p(data)\n","\n","def box_cox(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Box-Cox transform cannot be applied to non-positive values.\")\n","    transformed_data, _ = boxcox(data)\n","    return transformed_data\n","\n","# Data transformation\n","def standardize_data(data):\n","    standardized_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            standardized_data[column] = (data[column] - data[column].mean()) / data[column].std()\n","    return standardized_data\n","\n","def normalize_data(data):\n","    normalized_data = data.copy()\n","    for column in data.columns:\n","        min_val = data[column].min()\n","        max_val = data[column].max()\n","        if min_val != max_val:\n","            normalized_data[column] = (data[column] - min_val) / (max_val - min_val)\n","    return normalized_data\n","\n","def min_max_scaling(data):\n","    scaled_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            scaler = MinMaxScaler()\n","            scaled_data[column] = scaler.fit_transform(data[[column]])\n","    return scaled_data\n","\n","def robust_scaling(data):\n","    scaler = RobustScaler()\n","    scaled_data = scaler.fit_transform(data)\n","    scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n","    return scaled_data\n","\n","# Encode categorical features\n","def one_hot_encoding(data, categorical_columns):\n","    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","    encoded_cols = pd.DataFrame(encoder.fit_transform(data[categorical_columns]))\n","    encoded_cols.columns = encoder.get_feature_names(categorical_columns)\n","    data = data.drop(columns=categorical_columns)\n","    data = pd.concat([data, encoded_cols], axis=1)\n","    return data\n","\n","def label_encoding(data, categorical_columns):\n","    label_encoder = LabelEncoder()\n","    for column in categorical_columns:\n","        data[column] = label_encoder.fit_transform(data[column])\n","    return data\n","\n","# PCA\n","def pca_reduction(data, n_components=2):\n","    pca = PCA(n_components=n_components)\n","    reduced_data = pca.fit_transform(data)\n","    reduced_data = pd.DataFrame(reduced_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n","    return reduced_data"],"metadata":{"id":"8Bxyp-ufZaKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#automatic data preprocessing\n","import numpy as np\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n","from sklearn.decomposition import PCA\n","from scipy.stats import boxcox\n","from sklearn.impute import SimpleImputer\n","\n","# Load dataset\n","dataset = pd.read_csv('dp_data1.csv')\n","dataset\n","\n","print(\"Column indexes:\")\n","for i, column in enumerate(dataset.columns):\n","    print(f\"{i} : {column}\")\n","\n","target_index = int(input(\"Enter the index of the target variable: \"))\n","feature_indexes_str = input(\"Enter the indexes of the features (comma-separated): \")\n","feature_indexes = [int(idx.strip()) for idx in feature_indexes_str.split(',')]\n","\n","target_variable = dataset.columns[target_index]\n","features = [dataset.columns[idx] for idx in feature_indexes]\n","print()\n","\n","data = dataset[features + [target_variable]]\n","data\n","\n","# Data split\n","def split_data(dataset, target_variable):\n","    X = dataset.drop(columns=[target_variable])\n","    y = dataset[target_variable]\n","    return train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Handle missing values\n","def mean_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.mean())\n","    else:\n","        return data\n","\n","def median_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.median())\n","    else:\n","        return data\n","\n","def forward_fill(data):\n","    if data.isnull().any().any():\n","        return data.ffill()\n","    else:\n","        return data\n","\n","def backward_fill(data):\n","    if data.isnull().any().any():\n","        return data.bfill()\n","    else:\n","        return data\n","\n","def delete_missing_values(data):\n","    if data.isnull().any().any():\n","        return data.dropna()\n","    else:\n","        return data\n","\n","# Handle outliers and/or skewness\n","def winsorize(data, lower_percentile=5, upper_percentile=95):\n","    if any(data.apply(lambda x: np.any(x < np.percentile(x, lower_percentile)) or np.any(x > np.percentile(x, upper_percentile)))):\n","        lower_bound = np.percentile(data, lower_percentile)\n","        upper_bound = np.percentile(data, upper_percentile)\n","        data[data < lower_bound] = lower_bound\n","        data[data > upper_bound] = upper_bound\n","        return data\n","    else:\n","        return data\n","\n","def log_transform(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Log transform cannot be applied to non-positive values.\")\n","    return np.log1p(data)\n","\n","def box_cox(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Box-Cox transform cannot be applied to non-positive values.\")\n","    transformed_data, _ = boxcox(data)\n","    return transformed_data\n","\n","# Data transformation\n","def standardize_data(data):\n","    standardized_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            standardized_data[column] = (data[column] - data[column].mean()) / data[column].std()\n","    return standardized_data\n","\n","def normalize_data(data):\n","    normalized_data = data.copy()\n","    for column in data.columns:\n","        min_val = data[column].min()\n","        max_val = data[column].max()\n","        if min_val != max_val:\n","            normalized_data[column] = (data[column] - min_val) / (max_val - min_val)\n","    return normalized_data\n","\n","def min_max_scaling(data):\n","    scaled_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            scaler = MinMaxScaler()\n","            scaled_data[column] = scaler.fit_transform(data[[column]])\n","    return scaled_data\n","\n","def robust_scaling(data):\n","    scaler = RobustScaler()\n","    scaled_data = scaler.fit_transform(data)\n","    scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n","    return scaled_data\n","\n","# Encode categorical features\n","def one_hot_encoding(data, categorical_columns):\n","    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","    encoded_cols = pd.DataFrame(encoder.fit_transform(data[categorical_columns]))\n","    encoded_cols.columns = encoder.get_feature_names(categorical_columns)\n","    data = data.drop(columns=categorical_columns)\n","    data = pd.concat([data, encoded_cols], axis=1)\n","    return data\n","\n","def label_encoding(data, categorical_columns):\n","    label_encoder = LabelEncoder()\n","    for column in categorical_columns:\n","        data[column] = label_encoder.fit_transform(data[column])\n","    return data\n","\n","# PCA\n","def pca_reduction(data, n_components=2):\n","    pca = PCA(n_components=n_components)\n","    reduced_data = pca.fit_transform(data)\n","    reduced_data = pd.DataFrame(reduced_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n","    return reduced_data\n","\n","#types of features\n","def categorize_features(data):\n","    numerical_features = []\n","    categorical_features = []\n","\n","    for column in data.columns:\n","        if pd.api.types.is_numeric_dtype(data[column]):\n","            numerical_features.append(column)\n","        elif pd.api.types.is_object_dtype(data[column]):\n","            categorical_features.append(column)\n","\n","    return numerical_features, categorical_features\n","\n","#techniques selection\n","def determine_preprocessing_steps(data):\n","    techniques = []\n","    preprocessing_steps = {\n","        'imputation': False,\n","        'outlier_handling': False,\n","        'transformation': False,\n","        'encoding': False\n","    }\n","\n","    # Check for missing values\n","    if data.isnull().any().any():\n","        preprocessing_steps['imputation'] = True\n","        # Append imputation techniques\n","        techniques.extend(['mean_imputation', 'median_imputation', 'forward_fill', 'backward_fill', 'delete_missing_values'])\n","\n","    # Check for outliers\n","    # You can add more sophisticated outlier detection techniques here if needed\n","    if data.select_dtypes(include=['number']).apply(lambda x: x.skew()).abs().max() > 2:\n","        preprocessing_steps['outlier_handling'] = True\n","        # Append outlier handling techniques\n","        techniques.append('winsorize')\n","\n","    # Check if transformation is necessary (e.g., log transform for skewed data)\n","    if data.select_dtypes(include=['number']).apply(lambda x: x.skew()).abs().max() > 0.5:\n","        preprocessing_steps['transformation'] = True\n","        # Append transformation techniques\n","        techniques.extend(['log_transform', 'box_cox'])\n","\n","    # Check if encoding is required (for categorical features)\n","    if len(data.select_dtypes(include=['object', 'category']).columns) > 0:\n","        preprocessing_steps['encoding'] = True\n","        # Append encoding techniques\n","        techniques.extend(['one_hot_encoding', 'label_encoding'])\n","\n","    return preprocessing_steps, techniques\n","\n","# Modify preprocess_data function to take target_variable as input\n","def preprocess_data(data, target_variable):\n","    # Feature categorization\n","    numerical_features, categorical_features = categorize_features(data)\n","\n","    # Determine preprocessing steps\n","    preprocessing_steps, techniques = determine_preprocessing_steps(data)\n","\n","    # Initialize an empty list to store preprocessing pipelines\n","    preprocessing_pipelines = []\n","\n","    # If imputation is needed\n","    if preprocessing_steps['imputation']:\n","        for technique in ['mean_imputation', 'median_imputation', 'forward_fill', 'backward_fill', 'delete_missing_values']:\n","            if technique in techniques:\n","                pipeline_steps = [('imputation', globals()[technique])]\n","                preprocessing_pipelines.append(('Imputation: ' + technique, Pipeline(pipeline_steps)))\n","\n","    # If outlier handling is needed\n","    if preprocessing_steps['outlier_handling']:\n","        for technique in ['winsorize']:\n","            if technique in techniques:\n","                pipeline_steps = [('outlier_handling', globals()[technique])]\n","                preprocessing_pipelines.append(('Outlier Handling: ' + technique, Pipeline(pipeline_steps)))\n","\n","    # If transformation is needed\n","    if preprocessing_steps['transformation']:\n","        for technique in ['log_transform', 'box_cox']:\n","            if technique in techniques:\n","                pipeline_steps = [('transformation', globals()[technique])]\n","                preprocessing_pipelines.append(('Transformation: ' + technique, Pipeline(pipeline_steps)))\n","\n","    # If encoding is needed\n","    # If encoding is needed\n","    if preprocessing_steps['encoding']:\n","        if categorical_features:  # Check if there are categorical features\n","            for technique in ['one_hot_encoding', 'label_encoding']:\n","                if technique in techniques:\n","                    if technique == 'one_hot_encoding':\n","                        pipeline_steps = [('encoding', globals()[technique](categorical_columns))]\n","                    else:\n","                        pipeline_steps = [('encoding', globals()[technique](categorical_features))]\n","                    preprocessing_pipelines.append(('Encoding: ' + technique, Pipeline(pipeline_steps)))\n","    else:\n","      preprocessing_steps['encoding'] = False  # No categorical features to encode\n","\n","\n","    # No preprocessing required, use raw data\n","    pipeline_steps = [('none', None)]\n","    preprocessing_pipelines.append(('None', Pipeline(pipeline_steps)))\n","\n","    # Evaluate each preprocessing pipeline using cross-validation\n","    for name, pipeline in preprocessing_pipelines:\n","        print(f\"\\nEvaluation results for preprocessing pipeline: {name}\")\n","        preprocessing_data = dataset.copy()\n","\n","        # Apply preprocessing steps\n","        if pipeline is not None:\n","            X_train, X_test, y_train, y_test = split_data(preprocessing_data, target_variable)\n","            pipeline.fit(X_train, y_train)\n","            X_train_transformed = pipeline.transform(X_train)\n","            scores = cross_val_score(RandomForestClassifier(), X_train_transformed, y_train, cv=5)\n","            print(\"Mean Cross-Validation Accuracy:\", np.mean(scores))\n","        else:\n","            X_train, X_test, y_train, y_test = split_data(preprocessing_data, target_variable)\n","            scores = cross_val_score(RandomForestClassifier(), X_train, y_train, cv=5)\n","            print(\"Mean Cross-Validation Accuracy (No Preprocessing):\", np.mean(scores))\n","\n","# Example usage\n","target_variable = data.columns[-1]  # Assuming the target variable is the last column\n","preprocess_data(data, target_variable)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":801},"id":"iUnn087Nr9U_","executionInfo":{"status":"error","timestamp":1709212451081,"user_tz":-330,"elapsed":23274,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"eabf0311-30a3-43e2-973c-91ef5211078c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Column indexes:\n","0 : Feature_1\n","1 : Feature_2\n","2 : Feature_3\n","3 : Feature_4\n","4 : Feature_5\n","5 : Feature_6\n","6 : Feature_7\n","7 : Feature_8\n","8 : Feature_9\n","9 : Feature_10\n","10 : Feature_11\n","11 : Feature_12\n","12 : Feature_13\n","13 : Feature_14\n","14 : Feature_15\n","15 : Feature_16\n","16 : Feature_17\n","17 : Feature_18\n","18 : Feature_19\n","19 : Feature_20\n","20 : Feature_21\n","21 : Feature_22\n","22 : Feature_23\n","23 : Feature_24\n","24 : Feature_25\n","Enter the index of the target variable: 10\n","Enter the indexes of the features (comma-separated): 0,1,2,3,4,5,6,7,8,9\n","\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'categorical_columns' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-8da454108995>\u001b[0m in \u001b[0;36m<cell line: 265>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0mtarget_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming the target variable is the last column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-8da454108995>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data, target_variable)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtechnique\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtechniques\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtechnique\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'one_hot_encoding'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                         \u001b[0mpipeline_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtechnique\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0mpipeline_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtechnique\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'categorical_columns' is not defined"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n","from sklearn.decomposition import PCA\n","from scipy.stats import boxcox\n","from sklearn.impute import SimpleImputer\n","\n","# Load dataset\n","dataset = pd.read_csv('dp_data1.csv')\n","\n","def categorize_columns(dataset):\n","    numerical_columns = []\n","    categorical_columns = []\n","\n","    for column in dataset.columns:\n","        if dataset[column].dtype in ['int64', 'float64']:\n","            numerical_columns.append(column)\n","        else:\n","            categorical_columns.append(column)\n","\n","    return numerical_columns, categorical_columns\n","\n","def split_data(dataset, target_variable):\n","    X = dataset.drop(columns=[target_variable])\n","    y = dataset[target_variable]\n","    return train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","def mean_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.mean())\n","    else:\n","        return data\n","\n","def median_imputation(data):\n","    if data.isnull().any().any():\n","        return data.fillna(data.median())\n","    else:\n","        return data\n","\n","def forward_fill(data):\n","    if data.isnull().any().any():\n","        return data.ffill()\n","    else:\n","        return data\n","\n","def backward_fill(data):\n","    if data.isnull().any().any():\n","        return data.bfill()\n","    else:\n","        return data\n","\n","def delete_missing_values(data):\n","    if data.isnull().any().any():\n","        return data.dropna()\n","    else:\n","        return data\n","\n","def winsorize(data, lower_percentile=5, upper_percentile=95):\n","    if any(data.apply(lambda x: np.any(x < np.percentile(x, lower_percentile)) or np.any(x > np.percentile(x, upper_percentile)))):\n","        lower_bound = np.percentile(data, lower_percentile)\n","        upper_bound = np.percentile(data, upper_percentile)\n","        data[data < lower_bound] = lower_bound\n","        data[data > upper_bound] = upper_bound\n","        return data\n","    else:\n","        return data\n","\n","def log_transform(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Log transform cannot be applied to non-positive values.\")\n","    return np.log1p(data)\n","\n","def box_cox(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Box-Cox transform cannot be applied to non-positive values.\")\n","    transformed_data, _ = boxcox(data)\n","    return transformed_data\n","\n","def standardize_data(data):\n","    standardized_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            standardized_data[column] = (data[column] - data[column].mean()) / data[column].std()\n","    return standardized_data\n","\n","def normalize_data(data):\n","    normalized_data = data.copy()\n","    for column in data.columns:\n","        min_val = data[column].min()\n","        max_val = data[column].max()\n","        if min_val != max_val:\n","            normalized_data[column] = (data[column] - min_val) / (max_val - min_val)\n","    return normalized_data\n","\n","def min_max_scaling(data):\n","    scaled_data = data.copy()\n","    for column in data.columns:\n","        if data[column].dtype in [int, float] and data[column].std() != 0:\n","            scaler = MinMaxScaler()\n","            scaled_data[column] = scaler.fit_transform(data[[column]])\n","    return scaled_data\n","\n","def robust_scaling(data):\n","    scaler = RobustScaler()\n","    scaled_data = scaler.fit_transform(data)\n","    scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n","    return scaled_data\n","\n","def one_hot_encoding(data, categorical_columns):\n","    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","    encoded_cols = pd.DataFrame(encoder.fit_transform(data[categorical_columns]))\n","    encoded_cols.columns = encoder.get_feature_names(categorical_columns)\n","    data = data.drop(columns=categorical_columns)\n","    data = pd.concat([data, encoded_cols], axis=1)\n","    return data\n","\n","def label_encoding(data, categorical_columns):\n","    label_encoder = LabelEncoder()\n","    for column in categorical_columns:\n","        data[column] = label_encoder.fit_transform(data[column])\n","    return data\n","\n","def pca_reduction(data, n_components=2):\n","    pca = PCA(n_components=n_components)\n","    reduced_data = pca.fit_transform(data)\n","    reduced_data = pd.DataFrame(reduced_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n","    return reduced_data\n","\n","# Function to suggest preprocessing techniques\n","def required_preprocessing(data):\n","    preprocessing_techniques = []\n","\n","    # Check for missing values\n","    if data.isnull().values.any():\n","        preprocessing_techniques.append(\"Handle missing values (e.g., mean imputation, median imputation)\")\n","\n","    # Check for outliers\n","    numerical_columns, _ = categorize_columns(data)\n","    for column in numerical_columns:\n","        if data[column].max() > 3 * data[column].quantile(0.75) or data[column].min() < 3 * data[column].quantile(0.25):\n","            preprocessing_techniques.append(\"Handle outliers (e.g., Winsorization)\")\n","\n","    # Check for skewness\n","    skew_threshold = 1\n","    skewness = data[numerical_columns].skew()\n","    skewed_columns = skewness[abs(skewness) > skew_threshold].index.tolist()\n","    if skewed_columns:\n","        preprocessing_techniques.append(\"Handle skewness (e.g., log transformation, Box-Cox transformation)\")\n","\n","    # Check for categorical variables\n","    _, categorical_columns = categorize_columns(data)\n","    if len(categorical_columns) > 0:\n","        preprocessing_techniques.append(\"Encode categorical variables (e.g., one-hot encoding, label encoding)\")\n","\n","    return preprocessing_techniques\n","\n","# Get preprocessing suggestions\n","required_techniques = required_preprocessing(dataset)\n","\n","\n","class AutoDataPreprocessing:\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X\n","\n","    def fit_transform(self, X, y=None):\n","        return self.fit(X).transform(X)\n","\n","    def suggest_preprocessing(self, X, y=None):\n","        preprocessing_techniques = []\n","\n","        # Check for missing values\n","        if X.isnull().values.any():\n","            preprocessing_techniques.append(\"Handle missing values (e.g., mean imputation, median imputation)\")\n","\n","        # Check for outliers\n","        numerical_columns = X.select_dtypes(include=[np.number]).columns\n","        for column in numerical_columns:\n","            if self.detect_outliers(X[column]):\n","                preprocessing_techniques.append(\"Handle outliers (e.g., Winsorization)\")\n","\n","        # Check for skewness\n","        skew_threshold = 1\n","        skewness = X[numerical_columns].skew()\n","        skewed_columns = skewness[abs(skewness) > skew_threshold].index.tolist()\n","        if skewed_columns:\n","            preprocessing_techniques.append(\"Handle skewness (e.g., log transformation, Box-Cox transformation)\")\n","\n","        # Check for categorical variables\n","        categorical_columns = X.select_dtypes(exclude=[np.number]).columns\n","        if len(categorical_columns) > 0:\n","            preprocessing_techniques.append(\"Encode categorical variables (e.g., one-hot encoding)\")\n","\n","        return preprocessing_techniques\n","\n","    def detect_outliers(self, series, threshold=3):\n","        z_scores = (series - series.mean()) / series.std()\n","        return (z_scores.abs() > threshold).any()\n","\n","    def handle_missing_values(self, X):\n","        # Implement missing value handling techniques (e.g., mean imputation, median imputation)\n","        # Example:\n","        # imputer = SimpleImputer(strategy='mean')\n","        # X_filled = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n","        # return X_filled\n","        pass\n","\n","    def handle_outliers(self, X):\n","        # Implement outlier handling techniques (e.g., Winsorization)\n","        pass\n","\n","    def handle_skewness(self, X):\n","        # Implement skewness handling techniques (e.g., log transformation, Box-Cox transformation)\n","        pass\n","\n","    def encode_categorical_variables(self, X):\n","        # Implement categorical variable encoding techniques (e.g., one-hot encoding)\n","        # Example:\n","        # encoder = OneHotEncoder()\n","        # X_encoded = pd.DataFrame(encoder.fit_transform(X), columns=encoder.get_feature_names(X.columns))\n","        # return X_encoded\n","        pass\n","\n","# Load dataset\n","dataset = pd.read_csv('dp_data1.csv')\n","\n","# Separate features and target variable\n","X = dataset.drop(columns=['target_column'])\n","y = dataset['target_column']\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize AutoDataPreprocessing object\n","preprocessor = AutoDataPreprocessing()\n","\n","# Suggest preprocessing techniques\n","preprocessing_suggestions = preprocessor.suggest_preprocessing(X_train)\n","\n","print(\"Suggested preprocessing techniques:\")\n","for technique in preprocessing_suggestions:\n","    print(\"-\", technique)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mNQK6jQLZI-x","executionInfo":{"status":"ok","timestamp":1709219262245,"user_tz":-330,"elapsed":1885,"user":{"displayName":"Chinmay Kanawade","userId":"02462673596510628563"}},"outputId":"e7ccade2-a431-464d-e5fd-4bddf706a889"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing suggestions:\n","- Handle missing values (e.g., mean imputation, median imputation)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle outliers (e.g., Winsorization)\n","- Handle skewness (e.g., log transformation, Box-Cox transformation)\n","- Encode categorical variables (e.g., one-hot encoding, label encoding)\n"]}]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer, IterativeImputer\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import FunctionTransformer\n","import numpy as np\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n","from sklearn.decomposition import PCA\n","from scipy.stats import boxcox\n","\n","# Load dataset\n","dataset = pd.read_csv('dp_data1.csv')\n","\n","def categorize_columns(dataset):\n","    numerical_columns = []\n","    categorical_columns = []\n","\n","    for column in dataset.columns:\n","        if dataset[column].dtype in ['int64', 'float64']:\n","            numerical_columns.append(column)\n","        else:\n","            categorical_columns.append(column)\n","\n","    return numerical_columns, categorical_columns\n","\n","\n","def split_data(dataset, target_variable):\n","    X = dataset.drop(columns=[target_variable])\n","    y = dataset[target_variable]\n","    return train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","\n","\n","# Missing values\n","def handle_missing_values(dataset):\n","    num_missing = dataset.isnull().sum()\n","    num_columns = dataset.shape[1]\n","    if num_missing.sum() == 0:\n","        print(\"No missing values found\")\n","        return dataset\n","    else:\n","        imputer = None\n","        univariate_columns = [col for col in dataset.columns if dataset[col].isnull().any() and dataset[col].nunique() == 1]\n","        multivariate_columns = [col for col in dataset.columns if dataset[col].isnull().any() and dataset[col].nunique() > 1]\n","        if univariate_columns:\n","            imputer = SimpleImputer(strategy='mean')\n","            print(\"Using SimpleImputer for univariate columns:\", univariate_columns)\n","        if multivariate_columns:\n","            imputer = IterativeImputer()\n","            print(\"Using IterativeImputer for multivariate columns:\", multivariate_columns)\n","\n","        if imputer:\n","            dataset[univariate_columns + multivariate_columns] = imputer.fit_transform(dataset[univariate_columns + multivariate_columns])\n","            return dataset\n","        else:\n","            print(\"No suitable imputer found.\")\n","            return dataset\n","\n","# Outliers\n","def handle_outliers(dataset):\n","    dataset = dataset.apply(winsorize, axis=0)\n","    dataset = dataset.apply(log_transform, axis=0)\n","    return dataset\n","\n","def winsorize(data, lower_percentile=5, upper_percentile=95):\n","    if any(data.apply(lambda x: np.any(x < np.percentile(x, lower_percentile)) or np.any(x > np.percentile(x, upper_percentile)))):\n","        lower_bound = np.percentile(data, lower_percentile)\n","        upper_bound = np.percentile(data, upper_percentile)\n","        data[data < lower_bound] = lower_bound\n","        data[data > upper_bound] = upper_bound\n","        return data\n","    else:\n","        return data\n","\n","def log_transform(data):\n","    if any(data <= 0):\n","        raise ValueError(\"Log transform cannot be applied to non-positive values.\")\n","    return np.log1p(data)\n","\n","# Encoding categorical features\n","def encode_categorical_features(dataset):\n","    categorical_columns = dataset.select_dtypes(include=['object']).columns\n","    if len(categorical_columns) > 0:\n","        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","        encoded_data = encoder.fit_transform(dataset[categorical_columns])\n","        encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names(categorical_columns))\n","        dataset.drop(columns=categorical_columns, inplace=True)\n","        dataset = pd.concat([dataset, encoded_df], axis=1)\n","        return dataset\n","    else:\n","        print(\"No categorical features found.\")\n","        return dataset\n","\n","# Polynomial features\n","def add_polynomial_features(dataset, degree=2):\n","    polynomial_features = PolynomialFeatures(degree=degree)\n","    transformed_data = polynomial_features.fit_transform(dataset)\n","    return transformed_data\n","\n","# Normalization/Standardization/MinMaxScaler\n","def scale_features(dataset, scaler_type='standard'):\n","    if scaler_type == 'standard':\n","        scaler = StandardScaler()\n","    elif scaler_type == 'minmax':\n","        scaler = MinMaxScaler()\n","    else:\n","        raise ValueError(\"Invalid scaler_type. Use 'standard' or 'minmax'.\")\n","    scaled_data = scaler.fit_transform(dataset)\n","    return scaled_data\n","\n","#PCA\n","def pca_reduction(data, n_components=2):\n","    pca = PCA(n_components=n_components)\n","    reduced_data = pca.fit_transform(data)\n","    reduced_data = pd.DataFrame(reduced_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n","    return reduced_data\n","\n","\n","\n","# Function to suggest preprocessing techniques\n","def required_preprocessing(data):\n","    preprocessing_techniques = []\n","\n","    # Check for missing values\n","    if data.isnull().values.any():\n","        preprocessing_techniques.append(\"Handle missing values (e.g., mean imputation, median imputation)\")\n","\n","    # Check for outliers\n","    numerical_columns, _ = categorize_columns(data)\n","    for column in numerical_columns:\n","        if data[column].max() > 3 * data[column].quantile(0.75) or data[column].min() < 3 * data[column].quantile(0.25):\n","            preprocessing_techniques.append(\"Handle outliers (e.g., Winsorization)\")\n","\n","    # Check for skewness\n","    skew_threshold = 1\n","    skewness = data[numerical_columns].skew()\n","    skewed_columns = skewness[abs(skewness) > skew_threshold].index.tolist()\n","    if skewed_columns:\n","        preprocessing_techniques.append(\"Handle skewness (e.g., log transformation, Box-Cox transformation)\")\n","\n","    # Check for categorical variables\n","    _, categorical_columns = categorize_columns(data)\n","    if len(categorical_columns) > 0:\n","        preprocessing_techniques.append(\"Encode categorical variables (e.g., one-hot encoding, label encoding)\")\n","\n","    return preprocessing_techniques\n","\n","# Get preprocessing suggestions\n","required_techniques = required_preprocessing(dataset)\n","\n","# Define the preprocessing steps\n","preprocessing_steps = [\n","    ('missing_values', FunctionTransformer(handle_missing_values)),\n","    ('outliers', FunctionTransformer(handle_outliers)),\n","    ('categorical_encoding', FunctionTransformer(encode_categorical_features)),\n","    ('polynomial_features', FunctionTransformer(add_polynomial_features)),\n","    ('scaling', FunctionTransformer(scale_features)),\n","    ('pca', FunctionTransformer(pca_reduction))\n","]\n","\n","# Create the preprocessing pipeline\n","preprocessing_pipeline = Pipeline(steps=preprocessing_steps)\n","\n","# Apply the preprocessing pipeline to the dataset\n","preprocessed_data = preprocessing_pipeline.fit_transform(dataset)\n","\n","# Print the shape of the preprocessed data\n","print(\"Shape of preprocessed data:\", preprocessed_data.shape)\n"],"metadata":{"id":"gCBsUnNipp1b"},"execution_count":null,"outputs":[]}]}